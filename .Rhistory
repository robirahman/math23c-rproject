NCsd <- sd(NC$Weight) * sqrt( (nrow(NC)-1)/nrow(NC) ); NCsd
# 487.4943; confirmed w/given
# Create samples of 6 NC babies, and draw 10000 such samples without replacement
#From script 8D
n <- 6
N <- 10000; xx <- numeric(N); z <- numeric(N); b <- numeric(N); c <- numeric(N); d <- numeric(N)
for(i in 1:N){
x <- sample(NC$Weight, n)   #sample without replacement
xx[i] <- mean(x) #calculate the means of each
z[i] <- (xx[i] - NCmu)/(NCsd/sqrt(n)) #for question a: calculate the z-scores of each
b[i] <- sum(((x - NCmu)/NCsd)^2) # for question b
c[i] <- (sd(x)^2)*(n-1)/NCsd^2  #for question c, looking at sample variance
d[i] <- (xx[i] - NCmu)/(sd(x)/sqrt(n)) #for question d, studentizing.
#T-stat requires mult. by sample sd & div. by sqrt n
}
# a) Does Xbar - mu / (sigma/sqrt(n)) have an approximately normal standard distribution?
# Plot the z scores
hist(z)
hist(z, prob = TRUE)
curve(dnorm(x, 0, 1), add = TRUE) # Looks pretty standard normal with the curve, but let's test it.
qqnorm(z)
qqline(z) # The QQ plot also confirms that the z-scores follow a standard normal curve.
#Test using a chi-square test.
# Create standard normal bins based off quantiles of 10%
bins <- qnorm(.1*(0:10), 0, 1); bins
#From week 4 notes
# Sort the observed values (from our 10000 samples of size 6) into the normal bins
bincode <- cut(z, bins, labels = FALSE); bincode
#Now get the frequency of values in the normal bins
observed_inbins <- table(bincode) ; observed_inbins
# Get the expected values from these bin sorts
# There should be the same number of values (observations)
expected <- .1 * length(z); expected
# Compute Chi-square statistic.
chisq_z <- sum((observed_inbins - expected)^2/expected); chisq_z
#21.886
#p-value
pvalue_z <- pchisq(chisq_z, df = 8, lower.tail = FALSE); pvalue_z
#p-value_z = 0.001086484
# Using the p-test, we reject the null hypothesis that the z scores follow a standard normal
# distribution. If this did follow a std normal distribution, our observed results would
# have a .1% chance of happening.
# Question: the larger N is, the lower the p-value from the chi-square test,
#even though the observed zs' distribution appear. Am I missing something? Seems paradoxical.
#(b) Does the sum of the squares of the standardized samples (xbar - mu)/sigma
# have a distribution that is approximately chi-square with n = 6 degrees of freedom?
hist(b) #This looks like a chi-square distribution! Let's overlay the curve to see.
hist(b, probability = TRUE)
curve(dchisq(x, df = 6), col = "red", add = TRUE) #and the curve matches the histogram nicely
# (c) Multiply the sample variance by n - 1 and divide by the true variance.
# Does this have a distribution that is approximately chi-square with n - 1 = 5 degrees of freedom
hist(c) # Looks promising...
hist(c, probability = TRUE) # Still looks promising...
curve(dchisq(x, df = 5), col = "red", add = TRUE)
# This chi-square dist. with 5 df overlays the distribution of this statistic, based
# on the sample variances, pretty well! Looks like a good fit.
# (d) Studentizing the sample mean
hist(d) # Looks promising...
hist(d, probability = TRUE) # Still looks promising...
curve(dchisq(x, df = 5), col = "red", add = TRUE)
curve(dt(x, df = 5), col = "red", add = TRUE)
# Sharon Tai
# 04/09/2021
#R HW #9
seed(41)
#******************
#*Problem 1
#******************
NC <- read.csv("/Users/stai/Dropbox (Personal)/Work 2021/- Math 23c/Week 2/Week 2__export/NCBirths2004.csv"); head(NC)
NCmu <- mean(NC$Weight); NCmu
# 3448.26; confirmed w/given
NCsd <- sd(NC$Weight) * sqrt( (nrow(NC)-1)/nrow(NC) ); NCsd
# 487.4943; confirmed w/given
# Create samples of 6 NC babies, and draw 10000 such samples without replacement
#From script 8D
n <- 6
N <- 10000; xx <- numeric(N); z <- numeric(N); b <- numeric(N); c <- numeric(N); d <- numeric(N)
for(i in 1:N){
x <- sample(NC$Weight, n)   #sample without replacement
xx[i] <- mean(x) #calculate the means of each
z[i] <- (xx[i] - NCmu)/(NCsd/sqrt(n)) #for question a: calculate the z-scores of each
b[i] <- sum(((x - NCmu)/NCsd)^2) # for question b
c[i] <- (sd(x)^2)*(n-1)/NCsd^2  #for question c, looking at sample variance
d[i] <- (xx[i] - NCmu)/(sd(x)/sqrt(n)) #for question d, studentizing.
#T-stat requires mult. by sample sd & div. by sqrt n
}
# a) Does Xbar - mu / (sigma/sqrt(n)) have an approximately normal standard distribution?
# Plot the z scores
hist(z)
hist(z, prob = TRUE)
curve(dnorm(x, 0, 1), add = TRUE) # Looks pretty standard normal with the curve, but let's test it.
qqnorm(z)
qqline(z) # The QQ plot also confirms that the z-scores follow a standard normal curve.
#Test using a chi-square test.
# Create standard normal bins based off quantiles of 10%
bins <- qnorm(.1*(0:10), 0, 1); bins
#From week 4 notes
# Sort the observed values (from our 10000 samples of size 6) into the normal bins
bincode <- cut(z, bins, labels = FALSE); bincode
#Now get the frequency of values in the normal bins
observed_inbins <- table(bincode) ; observed_inbins
# Get the expected values from these bin sorts
# There should be the same number of values (observations)
expected <- .1 * length(z); expected
# Compute Chi-square statistic.
chisq_z <- sum((observed_inbins - expected)^2/expected); chisq_z
#21.886
#p-value
pvalue_z <- pchisq(chisq_z, df = 8, lower.tail = FALSE); pvalue_z
#p-value_z = 0.001086484
# Using the p-test, we reject the null hypothesis that the z scores follow a standard normal
# distribution. If this did follow a std normal distribution, our observed results would
# have a .1% chance of happening.
# Question: the larger N is, the lower the p-value from the chi-square test,
#even though the observed zs' distribution appear. Am I missing something? Seems paradoxical.
#(b) Does the sum of the squares of the standardized samples (xbar - mu)/sigma
# have a distribution that is approximately chi-square with n = 6 degrees of freedom?
hist(b) #This looks like a chi-square distribution! Let's overlay the curve to see.
hist(b, probability = TRUE)
curve(dchisq(x, df = 6), col = "red", add = TRUE) #and the curve matches the histogram nicely
# (c) Multiply the sample variance by n - 1 and divide by the true variance.
# Does this have a distribution that is approximately chi-square with n - 1 = 5 degrees of freedom
hist(c) # Looks promising...
hist(c, probability = TRUE) # Still looks promising...
curve(dchisq(x, df = 5), col = "red", add = TRUE)
# This chi-square dist. with 5 df overlays the distribution of this statistic, based
# on the sample variances, pretty well! Looks like a good fit.
# (d) Studentizing the sample mean
hist(d) # Looks promising...
hist(d, probability = TRUE) # Still looks promising...
curve(dt(x, df = 5), col = "magenta", add = TRUE)
# Quick test to confirm
library(ggplot2)
# sort calculated t-stat values and compute corresponding theoretical qtiles
d_sorted <- sort(d) ;
obs_ptiles <- (1:length(d)) / (1+length(d)) ;
t_qtiles <- qt(obs_ptiles, df = n-1, lower.tail = TRUE, log.p = FALSE) ;
# plot against theoretical quantiles
df <- data.frame(t_qtiles, d_sorted) ;
myplot <- ggplot(df, aes(x = t_qtiles, y = d_sorted))  +
geom_point(size=1, shape=23)
#myplot + geom_abline(intercept = 0, slope = 1, col = "blue")
myplot + geom_smooth(method = "lm", se = FALSE)
#*****************
#*Problem 2
#*****************
f <- function(x) dt(x,1)
integrate(f, -Inf, Inf)
#*****************
#*Problem 2
#*****************
f <- function(x) dt(x,1)
integrate(f, -Inf, Inf)
g <- function(x) x^2*dt(x,1)
integrate(f, -Inf, Inf)
integrate(g, -Inf, Inf)
g <- function(x) x * dt(x,1)
h <- function(x) x^2 * dt(x,1)
integrate(h, -Inf, Inf) - integrate(g, -Inf, Inf)^2
integrate(f, -Inf, 0)
integrate(g, 0, Inf)
integrate(g, -Inf, Inf)
integrate(g, 0, Inf)
integrate(g, -Inf, 0)
norm(5)
rt(5,1)
n <- 1000
N <- 1000; yy <- numeric(N);
for(i in 1:N){
y <- rt(n, 1)
yy[i] <- mean(y)
}
hist(yy)
integrate(g, -500, 200)
integrate(g, -300, 400)
integrate(g, -5000, 4000) # -0.2916611 with absolute error < 1.6e-06
integrate(g, -5000000, 4000) # -0.2916611 with absolute error < 1.6e-06
integrate(g, -1e12, 4000) # -0.2916611 with absolute error < 1.6e-06
log(12)
log(1e12)
log(1e12) - log(4e3)
# c)
f2 <- function(x) dt(x,2)
integrate(f2, -Inf, Inf) # confirmed this is 1
integrate(g2, -Inf, 0)
integrate(g2, 0, Inf)
g2 <- function(x) x*f2(x) # mu*pdf is the expectation
integrate(g2, -Inf, 0)
integrate(g2, 0, Inf)
h2 <- function(x) x^2 * f2(x)
integrate(h2, -Inf, Inf)
n <- 1000
N <- 1000; yy <- numeric(N);
for(i in 1:N){
y <- rt(n, 2)
yy[i] <- mean(y)
}
hist(yy)
h3 <- function(x) x^2 * f3(x) #because the expectation is actually 0
integrate(h3, -Inf, Inf) #maximum number of subdivisions reached
f3 <- function(x) dt(x,3)
integrate(f3, -Inf, Inf) # confirmed this is 1
g3 <- function(x) x*f3(x) # mu*pdf is the expectation
integrate(g3, -Inf, 0)
integrate(g3, 0, Inf)
#Indeed, these two have the same absolute value, and added would sum to 0.
h3 <- function(x) x^2 * f3(x) #because the expectation is actually 0
integrate(h3, -Inf, Inf) #maximum number of subdivisions reached
#f)
f12 <- function(x) dt(x,12)
integrate(f12, -Inf, Inf) # confirmed this is 1
#Indeed, these two have the same absolute value, and added would sum to 0.
h12 <- function(x) x^2 * f12(x) #because the expectation is actually 0
integrate(h12, -Inf, Inf) #3 with absolute error < 3.4e-07
f4_unscaled <- function(t) 1 / (4 + t^2)^(5/2)
integrate(f4_unscaled, -Inf, Inf)
1 / 0.08333333
f4 <- function(t) 12 / (4 + t^2)^(5/2)
integrate(f4, -Inf, Inf)
g4 <- function(t) t * f4
integrate(g4, -Inf, Inf)
f4 <- function(t) 12 / (4 + t^2)^(5/2)
integrate(f4, -Inf, Inf)
g4 <- function(t) t * f4(t)
integrate(g4, -Inf, Inf)
h4 <- function(t) t^2 * f4(t)
integrate(h4, -Inf, Inf)
# Sharon Tai
# 04/09/2021
#R HW #9
seed(41)
#******************
#*Problem 1
#******************
NC <- read.csv("/Users/stai/Dropbox (Personal)/Work 2021/- Math 23c/Week 2/Week 2__export/NCBirths2004.csv"); head(NC)
NCmu <- mean(NC$Weight); NCmu
# 3448.26; confirmed w/given
NCsd <- sd(NC$Weight) * sqrt( (nrow(NC)-1)/nrow(NC) ); NCsd
# 487.4943; confirmed w/given
# Create samples of 6 NC babies, and draw 10000 such samples without replacement
#From script 8D
n <- 6
N <- 10000; xx <- numeric(N); z <- numeric(N); b <- numeric(N); c <- numeric(N); d <- numeric(N)
for(i in 1:N){
x <- sample(NC$Weight, n)   #sample without replacement
xx[i] <- mean(x) #calculate the means of each
z[i] <- (xx[i] - NCmu)/(NCsd/sqrt(n)) #for question a: calculate the z-scores of each
b[i] <- sum(((x - NCmu)/NCsd)^2) # for question b
c[i] <- (sd(x)^2)*(n-1)/NCsd^2  #for question c, looking at sample variance
d[i] <- (xx[i] - NCmu)/(sd(x)/sqrt(n)) #for question d, studentizing.
#T-stat requires mult. by sample sd & div. by sqrt n
}
# a) Does Xbar - mu / (sigma/sqrt(n)) have an approximately normal standard distribution?
# Plot the z scores
hist(z)
hist(z, prob = TRUE)
curve(dnorm(x, 0, 1), add = TRUE) # Looks pretty standard normal with the curve, but let's test it.
qqnorm(z)
qqline(z) # The QQ plot also confirms that the z-scores follow a standard normal curve.
#Test using a chi-square test.
# Create standard normal bins based off quantiles of 10%
bins <- qnorm(.1*(0:10), 0, 1); bins
#From week 4 notes
# Sort the observed values (from our 10000 samples of size 6) into the normal bins
bincode <- cut(z, bins, labels = FALSE); bincode
#Now get the frequency of values in the normal bins
observed_inbins <- table(bincode) ; observed_inbins
# Get the expected values from these bin sorts
# There should be the same number of values (observations)
expected <- .1 * length(z); expected
# Compute Chi-square statistic.
chisq_z <- sum((observed_inbins - expected)^2/expected); chisq_z
chiSqTest <- dget("chiSqTest.R")
pwd
getwd()
setwd("~/Dropbox (Personal)/Work 2021/- Math 23c/term project")
chiSqTest <- dget("chiSqTest.R")
pvalue_z <- pchisq(chisq_z, df = 8, lower.tail = FALSE); pvalue_z
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
library(ggplot2)
library(zoo)
setwd("~/Dropbox (Personal)/Work 2021/- Math 23c/term project")
chiSqTest <- dget("chiSqTest.R")
setwd("/Users/stai/Math23cproject/math23c-rproject")
library(ggplot2)
library(zoo)
setwd("~/Dropbox (Personal)/Work 2021/- Math 23c/term project")
chiSqTest <- dget("chiSqTest.R")
setwd("/Users/stai/math23c-rproject")
projdata <- read.csv("/Users/stai/math23c-rproject/source_data/commodities data.csv", stringsAsFactors=FALSE); head(projdata)
HSY_data <- read.csv("/Users/stai/math23c-rproject/source_data/HSY.csv", stringsAsFactors=FALSE); head(HSY_data)
rec_indic <-read.csv("/Users/stai/math23c-rproject/source_data/monthly recession indicator.csv", stringsAsFactors=FALSE); head(rec_indic)
summary(rec_indic)
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC
# Create a dataframe
df_comm <- data.frame(projdata); head(df_comm)
df_comm$Gold..USD...ozt. <- as.numeric(gsub(",", "", df_comm$Gold..USD...ozt.)) ;
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC
rec_indic
df_comm$Month
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC(1:241)
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC[1:241]
# Change the month column to dates we can use to do calculations. Need the zoo package.
time = df_comm$Month
# Converts the column into humanly readable dates. The as.Date function allows R to actually operate on it.
df_comm$Date = as.Date(as.yearmon(time)) ;
# Use zoo to convert dataframe to time series (easier to define price changes)
df_ts = as.ts(read.zoo(df_comm, FUN = as.yearmon)) ;
plot(df_ts, df_comm$rec_indic)
plot(df_comm$Date, df_comm$rec_indic)
c(df_comm$Date, df_comm$rec_indic)
df_comm$Date
df_comm$rec_indic
df_comm$rec2001 <- df_comm$Date >= df_comm$Date[3] & df_comm$Date <= df_comm$Date[10]
plot(df_comm$Date, df_comm$rec2001)
df_comm$Date[where(df_comm$rec_indic == 1)]
df_comm$Date[which(df_comm$rec_indic == 1)]
which(df_comm$rec_indic == 1)
df_comm$rec2008 <- df_comm$Date >= df_comm$Date[84] & df_comm$Date <= df_comm$Date[101]
df_comm$rec2020 <- df_comm$Date >= df_comm$Date[230] & df_comm$Date <= df_comm$Date[241]
plot(df_comm$Date, df_comm$rec2008)
plot(df_comm$Date, df_comm$rec2020)
hist(df_comm$Gold..USD...ozt.[1:120])
hist(df_comm$Gold..USD...ozt.[121:240])
#histogram of purely values, not time series
hist(df_comm$Gold..USD...ozt., breaks=50)
hist(diff(log(df_comm$Gold..USD...ozt.[1:120])))
hist(diff(log(df_comm$Gold..USD...ozt.[121:240])))
# differences in price changes
hist(diff(diff(log(df_comm$Gold..USD...ozt.[1:120]))))
hist(diff(diff(log(df_comm$Gold..USD...ozt.[121:240]))))
# differences in price changes
gold_diff_diff1 <- hist(diff(diff(log(df_comm$Gold..USD...ozt.[1:120]))))
gold_diff_diff2 <- hist(diff(diff(log(df_comm$Gold..USD...ozt.[121:240]))))
# differences in price changes
gold_diff_diff1 <- diff(diff(log(df_comm$Gold..USD...ozt.[1:120])))
hist(gold_diff_diff1)
gold_diff_diff2 <- diff(diff(log(df_comm$Gold..USD...ozt.[121:240])))
hist(gold_diff_diff2)
#Gold distribution
pval_gold_diff_diff1 <- chiSqTest(gold_diff_diff1)
pval_gold_diff_diff2 <- chiSqTest(gold_diff_diff1)
pval_gold_diff_diff2 <- chiSqTest(gold_diff_diff2)
# First half
hist(df_comm$Gold..USD...ozt.[1:120])
# Second half
hist(df_comm$Gold..USD...ozt.[121:240])
#histogram of purely values, not time series
hist(df_comm$Gold..USD...ozt., breaks=50)
#Gold distribution
pval_gold_diff_diff1 <- chiSqTest(gold_diff_diff1)
# price changes/difference in prices
gold_price_change1 <- diff(log(df_comm$Gold..USD...ozt.[1:120]))
hist(gold_price_change1)
gold_price_change2 <- diff(log(df_comm$Gold..USD...ozt.[121:240]))
hist(gold_price_change2)
# differences in price changes
gold_diff_diff1 <- diff(diff(log(df_comm$Gold..USD...ozt.[1:120])))
hist(gold_diff_diff1)
gold_diff_diff2 <- diff(diff(log(df_comm$Gold..USD...ozt.[121:240])))
hist(gold_diff_diff2)
#Gold distribution
pval_gold_diff_diff1 <- chiSqTest(gold_diff_diff1)
pval_gold_diff_diff2 <- chiSqTest(gold_diff_diff2)
# QQ plots to see how distribution compares to normal distribution
qqnorm(gold_diff_diff1)
qqline(gold_diff_diff1)
qqnorm(gold_diff_diff2)
qqline(gold_diff_diff2)
# Price Changes
qqnorm(gold_price_change1)
qqline(gold_price_change2)
qqnorm(gold_price_change1)
# Price Changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
qqnorm(gold_price_change2)
qqline(gold_price_change2)
# Log Price Changes/percentage changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
qqnorm(gold_price_change2)
qqline(gold_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(gold_diff_diff1)
qqline(gold_diff_diff1)
qqnorm(gold_diff_diff2)
qqline(gold_diff_diff2)
#*****************
#*Beef
#*****************
hist(df_comm$Beef..USD...kg., breaks=50)
length(df_comm$Beef..USD...kg.)
#histogram of purely values, not time series
hist(df_comm$Beef..USD...kg.)
# First half of the dataset, which is ordered according to time
hist(df_comm$Beef..USD...kg.[1:120])
# Second half (of the time frame)
hist(df_comm$Beef..USD...kg.[121:240])
# price changes/difference in prices
beef_price_change1 <- diff(log(df_comm$Beef..USD...kg.[1:120]))
hist(beef_price_change1)
beef_price_change2 <- diff(log(df_comm$Beef..USD...kg.[121:240]))
hist(beef_price_change2)
# differences in price changes
beef_diff_diff1 <- diff(diff(log(df_comm$Beef..USD...kg.[1:120])))
hist(beef_diff_diff1)
beef_diff_diff2 <- diff(diff(log(df_comm$Beef..USD...kg.[121:240])))
beef(beef_diff_diff2)
hist(beef_diff_diff2)
#Beef distribution
pval_beef_diff_diff1 <- chiSqTest(beef_diff_diff1)
pval_beef_diff_diff2 <- chiSqTest(beef_diff_diff2)
# Log Price Changes/percentage changes
qqnorm(beef_price_change1)
qqline(beef_price_change1)
qqnorm(beef_price_change2)
qqline(beef_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(beef_diff_diff1)
qqline(beef_diff_diff1)
qqnorm(beef_diff_diff2)
qqline(beef_diff_diff2)
df_comm$Date[which(df_comm$rec_indic == 1)]
hist(df_comm$Gold..USD...ozt.(which(df_comm$rec_indic == 1)))
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)], breaks=50)
# Do we need daily data on this...
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$GBeef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)])
# Do we need daily data on this...
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)])
# Refined with more bins
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
# Do we need daily data on this...
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)])
# Refined with more bins
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
# Log Price Changes/percentage changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
# Log Price Changes/percentage changes
qqnorm(beef_price_change1)
qqline(beef_price_change1)
qqnorm(beef_price_change2)
qqline(beef_price_change2)
qqnorm(beef_price_change2)
qqline(beef_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(beef_diff_diff1)
qqline(beef_diff_diff1)
qqnorm(beef_diff_diff2)
qqline(beef_diff_diff2)
setwd("/Users/stai/math23c-rproject")
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)], breaks=50)
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)], breaks=50)
# Log Price Changes/percentage changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
qqnorm(gold_price_change2)
qqline(gold_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(gold_diff_diff1)
qqline(gold_diff_diff1)
qqnorm(gold_diff_diff2)
qqline(gold_diff_diff2)
# Log Price Changes/percentage changes
qqnorm(beef_price_change1)
qqline(beef_price_change1)
qqnorm(beef_price_change2) #Note what's happening in the right and left tails.
qqline(beef_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(beef_diff_diff1)
qqline(beef_diff_diff1)
qqnorm(beef_diff_diff2)
qqline(beef_diff_diff2) # Note what's happening in the right and left tails
