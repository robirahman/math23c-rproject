<<<<<<< HEAD
NCsd <- sd(NC$Weight) * sqrt( (nrow(NC)-1)/nrow(NC) ); NCsd
# 487.4943; confirmed w/given
# Create samples of 6 NC babies, and draw 10000 such samples without replacement
#From script 8D
n <- 6
N <- 10000; xx <- numeric(N); z <- numeric(N); b <- numeric(N); c <- numeric(N); d <- numeric(N)
for(i in 1:N){
x <- sample(NC$Weight, n)   #sample without replacement
xx[i] <- mean(x) #calculate the means of each
z[i] <- (xx[i] - NCmu)/(NCsd/sqrt(n)) #for question a: calculate the z-scores of each
b[i] <- sum(((x - NCmu)/NCsd)^2) # for question b
c[i] <- (sd(x)^2)*(n-1)/NCsd^2  #for question c, looking at sample variance
d[i] <- (xx[i] - NCmu)/(sd(x)/sqrt(n)) #for question d, studentizing.
#T-stat requires mult. by sample sd & div. by sqrt n
}
# a) Does Xbar - mu / (sigma/sqrt(n)) have an approximately normal standard distribution?
# Plot the z scores
hist(z)
hist(z, prob = TRUE)
curve(dnorm(x, 0, 1), add = TRUE) # Looks pretty standard normal with the curve, but let's test it.
qqnorm(z)
qqline(z) # The QQ plot also confirms that the z-scores follow a standard normal curve.
#Test using a chi-square test.
# Create standard normal bins based off quantiles of 10%
bins <- qnorm(.1*(0:10), 0, 1); bins
#From week 4 notes
# Sort the observed values (from our 10000 samples of size 6) into the normal bins
bincode <- cut(z, bins, labels = FALSE); bincode
#Now get the frequency of values in the normal bins
observed_inbins <- table(bincode) ; observed_inbins
# Get the expected values from these bin sorts
# There should be the same number of values (observations)
expected <- .1 * length(z); expected
# Compute Chi-square statistic.
chisq_z <- sum((observed_inbins - expected)^2/expected); chisq_z
#21.886
#p-value
pvalue_z <- pchisq(chisq_z, df = 8, lower.tail = FALSE); pvalue_z
#p-value_z = 0.001086484
# Using the p-test, we reject the null hypothesis that the z scores follow a standard normal
# distribution. If this did follow a std normal distribution, our observed results would
# have a .1% chance of happening.
# Question: the larger N is, the lower the p-value from the chi-square test,
#even though the observed zs' distribution appear. Am I missing something? Seems paradoxical.
#(b) Does the sum of the squares of the standardized samples (xbar - mu)/sigma
# have a distribution that is approximately chi-square with n = 6 degrees of freedom?
hist(b) #This looks like a chi-square distribution! Let's overlay the curve to see.
hist(b, probability = TRUE)
curve(dchisq(x, df = 6), col = "red", add = TRUE) #and the curve matches the histogram nicely
# (c) Multiply the sample variance by n - 1 and divide by the true variance.
# Does this have a distribution that is approximately chi-square with n - 1 = 5 degrees of freedom
hist(c) # Looks promising...
hist(c, probability = TRUE) # Still looks promising...
curve(dchisq(x, df = 5), col = "red", add = TRUE)
# This chi-square dist. with 5 df overlays the distribution of this statistic, based
# on the sample variances, pretty well! Looks like a good fit.
# (d) Studentizing the sample mean
hist(d) # Looks promising...
hist(d, probability = TRUE) # Still looks promising...
curve(dchisq(x, df = 5), col = "red", add = TRUE)
curve(dt(x, df = 5), col = "red", add = TRUE)
# Sharon Tai
# 04/09/2021
#R HW #9
seed(41)
#******************
#*Problem 1
#******************
NC <- read.csv("/Users/stai/Dropbox (Personal)/Work 2021/- Math 23c/Week 2/Week 2__export/NCBirths2004.csv"); head(NC)
NCmu <- mean(NC$Weight); NCmu
# 3448.26; confirmed w/given
NCsd <- sd(NC$Weight) * sqrt( (nrow(NC)-1)/nrow(NC) ); NCsd
# 487.4943; confirmed w/given
# Create samples of 6 NC babies, and draw 10000 such samples without replacement
#From script 8D
n <- 6
N <- 10000; xx <- numeric(N); z <- numeric(N); b <- numeric(N); c <- numeric(N); d <- numeric(N)
for(i in 1:N){
x <- sample(NC$Weight, n)   #sample without replacement
xx[i] <- mean(x) #calculate the means of each
z[i] <- (xx[i] - NCmu)/(NCsd/sqrt(n)) #for question a: calculate the z-scores of each
b[i] <- sum(((x - NCmu)/NCsd)^2) # for question b
c[i] <- (sd(x)^2)*(n-1)/NCsd^2  #for question c, looking at sample variance
d[i] <- (xx[i] - NCmu)/(sd(x)/sqrt(n)) #for question d, studentizing.
#T-stat requires mult. by sample sd & div. by sqrt n
}
# a) Does Xbar - mu / (sigma/sqrt(n)) have an approximately normal standard distribution?
# Plot the z scores
hist(z)
hist(z, prob = TRUE)
curve(dnorm(x, 0, 1), add = TRUE) # Looks pretty standard normal with the curve, but let's test it.
qqnorm(z)
qqline(z) # The QQ plot also confirms that the z-scores follow a standard normal curve.
#Test using a chi-square test.
# Create standard normal bins based off quantiles of 10%
bins <- qnorm(.1*(0:10), 0, 1); bins
#From week 4 notes
# Sort the observed values (from our 10000 samples of size 6) into the normal bins
bincode <- cut(z, bins, labels = FALSE); bincode
#Now get the frequency of values in the normal bins
observed_inbins <- table(bincode) ; observed_inbins
# Get the expected values from these bin sorts
# There should be the same number of values (observations)
expected <- .1 * length(z); expected
# Compute Chi-square statistic.
chisq_z <- sum((observed_inbins - expected)^2/expected); chisq_z
#21.886
#p-value
pvalue_z <- pchisq(chisq_z, df = 8, lower.tail = FALSE); pvalue_z
#p-value_z = 0.001086484
# Using the p-test, we reject the null hypothesis that the z scores follow a standard normal
# distribution. If this did follow a std normal distribution, our observed results would
# have a .1% chance of happening.
# Question: the larger N is, the lower the p-value from the chi-square test,
#even though the observed zs' distribution appear. Am I missing something? Seems paradoxical.
#(b) Does the sum of the squares of the standardized samples (xbar - mu)/sigma
# have a distribution that is approximately chi-square with n = 6 degrees of freedom?
hist(b) #This looks like a chi-square distribution! Let's overlay the curve to see.
hist(b, probability = TRUE)
curve(dchisq(x, df = 6), col = "red", add = TRUE) #and the curve matches the histogram nicely
# (c) Multiply the sample variance by n - 1 and divide by the true variance.
# Does this have a distribution that is approximately chi-square with n - 1 = 5 degrees of freedom
hist(c) # Looks promising...
hist(c, probability = TRUE) # Still looks promising...
curve(dchisq(x, df = 5), col = "red", add = TRUE)
# This chi-square dist. with 5 df overlays the distribution of this statistic, based
# on the sample variances, pretty well! Looks like a good fit.
# (d) Studentizing the sample mean
hist(d) # Looks promising...
hist(d, probability = TRUE) # Still looks promising...
curve(dt(x, df = 5), col = "magenta", add = TRUE)
# Quick test to confirm
library(ggplot2)
# sort calculated t-stat values and compute corresponding theoretical qtiles
d_sorted <- sort(d) ;
obs_ptiles <- (1:length(d)) / (1+length(d)) ;
t_qtiles <- qt(obs_ptiles, df = n-1, lower.tail = TRUE, log.p = FALSE) ;
# plot against theoretical quantiles
df <- data.frame(t_qtiles, d_sorted) ;
myplot <- ggplot(df, aes(x = t_qtiles, y = d_sorted))  +
geom_point(size=1, shape=23)
#myplot + geom_abline(intercept = 0, slope = 1, col = "blue")
myplot + geom_smooth(method = "lm", se = FALSE)
#*****************
#*Problem 2
#*****************
f <- function(x) dt(x,1)
integrate(f, -Inf, Inf)
#*****************
#*Problem 2
#*****************
f <- function(x) dt(x,1)
integrate(f, -Inf, Inf)
g <- function(x) x^2*dt(x,1)
integrate(f, -Inf, Inf)
integrate(g, -Inf, Inf)
g <- function(x) x * dt(x,1)
h <- function(x) x^2 * dt(x,1)
integrate(h, -Inf, Inf) - integrate(g, -Inf, Inf)^2
integrate(f, -Inf, 0)
integrate(g, 0, Inf)
integrate(g, -Inf, Inf)
integrate(g, 0, Inf)
integrate(g, -Inf, 0)
norm(5)
rt(5,1)
n <- 1000
N <- 1000; yy <- numeric(N);
for(i in 1:N){
y <- rt(n, 1)
yy[i] <- mean(y)
}
hist(yy)
integrate(g, -500, 200)
integrate(g, -300, 400)
integrate(g, -5000, 4000) # -0.2916611 with absolute error < 1.6e-06
integrate(g, -5000000, 4000) # -0.2916611 with absolute error < 1.6e-06
integrate(g, -1e12, 4000) # -0.2916611 with absolute error < 1.6e-06
log(12)
log(1e12)
log(1e12) - log(4e3)
# c)
f2 <- function(x) dt(x,2)
integrate(f2, -Inf, Inf) # confirmed this is 1
integrate(g2, -Inf, 0)
integrate(g2, 0, Inf)
g2 <- function(x) x*f2(x) # mu*pdf is the expectation
integrate(g2, -Inf, 0)
integrate(g2, 0, Inf)
h2 <- function(x) x^2 * f2(x)
integrate(h2, -Inf, Inf)
n <- 1000
N <- 1000; yy <- numeric(N);
for(i in 1:N){
y <- rt(n, 2)
yy[i] <- mean(y)
}
hist(yy)
h3 <- function(x) x^2 * f3(x) #because the expectation is actually 0
integrate(h3, -Inf, Inf) #maximum number of subdivisions reached
f3 <- function(x) dt(x,3)
integrate(f3, -Inf, Inf) # confirmed this is 1
g3 <- function(x) x*f3(x) # mu*pdf is the expectation
integrate(g3, -Inf, 0)
integrate(g3, 0, Inf)
#Indeed, these two have the same absolute value, and added would sum to 0.
h3 <- function(x) x^2 * f3(x) #because the expectation is actually 0
integrate(h3, -Inf, Inf) #maximum number of subdivisions reached
#f)
f12 <- function(x) dt(x,12)
integrate(f12, -Inf, Inf) # confirmed this is 1
#Indeed, these two have the same absolute value, and added would sum to 0.
h12 <- function(x) x^2 * f12(x) #because the expectation is actually 0
integrate(h12, -Inf, Inf) #3 with absolute error < 3.4e-07
f4_unscaled <- function(t) 1 / (4 + t^2)^(5/2)
integrate(f4_unscaled, -Inf, Inf)
1 / 0.08333333
f4 <- function(t) 12 / (4 + t^2)^(5/2)
integrate(f4, -Inf, Inf)
g4 <- function(t) t * f4
integrate(g4, -Inf, Inf)
f4 <- function(t) 12 / (4 + t^2)^(5/2)
integrate(f4, -Inf, Inf)
g4 <- function(t) t * f4(t)
integrate(g4, -Inf, Inf)
h4 <- function(t) t^2 * f4(t)
integrate(h4, -Inf, Inf)
# Sharon Tai
# 04/09/2021
#R HW #9
seed(41)
#******************
#*Problem 1
#******************
NC <- read.csv("/Users/stai/Dropbox (Personal)/Work 2021/- Math 23c/Week 2/Week 2__export/NCBirths2004.csv"); head(NC)
NCmu <- mean(NC$Weight); NCmu
# 3448.26; confirmed w/given
NCsd <- sd(NC$Weight) * sqrt( (nrow(NC)-1)/nrow(NC) ); NCsd
# 487.4943; confirmed w/given
# Create samples of 6 NC babies, and draw 10000 such samples without replacement
#From script 8D
n <- 6
N <- 10000; xx <- numeric(N); z <- numeric(N); b <- numeric(N); c <- numeric(N); d <- numeric(N)
for(i in 1:N){
x <- sample(NC$Weight, n)   #sample without replacement
xx[i] <- mean(x) #calculate the means of each
z[i] <- (xx[i] - NCmu)/(NCsd/sqrt(n)) #for question a: calculate the z-scores of each
b[i] <- sum(((x - NCmu)/NCsd)^2) # for question b
c[i] <- (sd(x)^2)*(n-1)/NCsd^2  #for question c, looking at sample variance
d[i] <- (xx[i] - NCmu)/(sd(x)/sqrt(n)) #for question d, studentizing.
#T-stat requires mult. by sample sd & div. by sqrt n
}
# a) Does Xbar - mu / (sigma/sqrt(n)) have an approximately normal standard distribution?
# Plot the z scores
hist(z)
hist(z, prob = TRUE)
curve(dnorm(x, 0, 1), add = TRUE) # Looks pretty standard normal with the curve, but let's test it.
qqnorm(z)
qqline(z) # The QQ plot also confirms that the z-scores follow a standard normal curve.
#Test using a chi-square test.
# Create standard normal bins based off quantiles of 10%
bins <- qnorm(.1*(0:10), 0, 1); bins
#From week 4 notes
# Sort the observed values (from our 10000 samples of size 6) into the normal bins
bincode <- cut(z, bins, labels = FALSE); bincode
#Now get the frequency of values in the normal bins
observed_inbins <- table(bincode) ; observed_inbins
# Get the expected values from these bin sorts
# There should be the same number of values (observations)
expected <- .1 * length(z); expected
# Compute Chi-square statistic.
chisq_z <- sum((observed_inbins - expected)^2/expected); chisq_z
chiSqTest <- dget("chiSqTest.R")
pwd
getwd()
setwd("~/Dropbox (Personal)/Work 2021/- Math 23c/term project")
chiSqTest <- dget("chiSqTest.R")
pvalue_z <- pchisq(chisq_z, df = 8, lower.tail = FALSE); pvalue_z
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
chiSqTest <- dget("chiSqTest.R")
chiSqTest(z)
library(ggplot2)
library(zoo)
setwd("~/Dropbox (Personal)/Work 2021/- Math 23c/term project")
chiSqTest <- dget("chiSqTest.R")
setwd("/Users/stai/Math23cproject/math23c-rproject")
library(ggplot2)
library(zoo)
setwd("~/Dropbox (Personal)/Work 2021/- Math 23c/term project")
chiSqTest <- dget("chiSqTest.R")
setwd("/Users/stai/math23c-rproject")
projdata <- read.csv("/Users/stai/math23c-rproject/source_data/commodities data.csv", stringsAsFactors=FALSE); head(projdata)
HSY_data <- read.csv("/Users/stai/math23c-rproject/source_data/HSY.csv", stringsAsFactors=FALSE); head(HSY_data)
rec_indic <-read.csv("/Users/stai/math23c-rproject/source_data/monthly recession indicator.csv", stringsAsFactors=FALSE); head(rec_indic)
summary(rec_indic)
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC
# Create a dataframe
df_comm <- data.frame(projdata); head(df_comm)
df_comm$Gold..USD...ozt. <- as.numeric(gsub(",", "", df_comm$Gold..USD...ozt.)) ;
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC
rec_indic
df_comm$Month
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC(1:241)
# Append the recession indicator
df_comm$rec_indic <- rec_indic$USREC[1:241]
# Change the month column to dates we can use to do calculations. Need the zoo package.
time = df_comm$Month
# Converts the column into humanly readable dates. The as.Date function allows R to actually operate on it.
df_comm$Date = as.Date(as.yearmon(time)) ;
# Use zoo to convert dataframe to time series (easier to define price changes)
df_ts = as.ts(read.zoo(df_comm, FUN = as.yearmon)) ;
plot(df_ts, df_comm$rec_indic)
plot(df_comm$Date, df_comm$rec_indic)
c(df_comm$Date, df_comm$rec_indic)
df_comm$Date
df_comm$rec_indic
df_comm$rec2001 <- df_comm$Date >= df_comm$Date[3] & df_comm$Date <= df_comm$Date[10]
plot(df_comm$Date, df_comm$rec2001)
df_comm$Date[where(df_comm$rec_indic == 1)]
df_comm$Date[which(df_comm$rec_indic == 1)]
which(df_comm$rec_indic == 1)
df_comm$rec2008 <- df_comm$Date >= df_comm$Date[84] & df_comm$Date <= df_comm$Date[101]
df_comm$rec2020 <- df_comm$Date >= df_comm$Date[230] & df_comm$Date <= df_comm$Date[241]
plot(df_comm$Date, df_comm$rec2008)
plot(df_comm$Date, df_comm$rec2020)
hist(df_comm$Gold..USD...ozt.[1:120])
hist(df_comm$Gold..USD...ozt.[121:240])
#histogram of purely values, not time series
hist(df_comm$Gold..USD...ozt., breaks=50)
hist(diff(log(df_comm$Gold..USD...ozt.[1:120])))
hist(diff(log(df_comm$Gold..USD...ozt.[121:240])))
# differences in price changes
hist(diff(diff(log(df_comm$Gold..USD...ozt.[1:120]))))
hist(diff(diff(log(df_comm$Gold..USD...ozt.[121:240]))))
# differences in price changes
gold_diff_diff1 <- hist(diff(diff(log(df_comm$Gold..USD...ozt.[1:120]))))
gold_diff_diff2 <- hist(diff(diff(log(df_comm$Gold..USD...ozt.[121:240]))))
# differences in price changes
gold_diff_diff1 <- diff(diff(log(df_comm$Gold..USD...ozt.[1:120])))
hist(gold_diff_diff1)
gold_diff_diff2 <- diff(diff(log(df_comm$Gold..USD...ozt.[121:240])))
hist(gold_diff_diff2)
#Gold distribution
pval_gold_diff_diff1 <- chiSqTest(gold_diff_diff1)
pval_gold_diff_diff2 <- chiSqTest(gold_diff_diff1)
pval_gold_diff_diff2 <- chiSqTest(gold_diff_diff2)
# First half
hist(df_comm$Gold..USD...ozt.[1:120])
# Second half
hist(df_comm$Gold..USD...ozt.[121:240])
#histogram of purely values, not time series
hist(df_comm$Gold..USD...ozt., breaks=50)
#Gold distribution
pval_gold_diff_diff1 <- chiSqTest(gold_diff_diff1)
# price changes/difference in prices
gold_price_change1 <- diff(log(df_comm$Gold..USD...ozt.[1:120]))
hist(gold_price_change1)
gold_price_change2 <- diff(log(df_comm$Gold..USD...ozt.[121:240]))
hist(gold_price_change2)
# differences in price changes
gold_diff_diff1 <- diff(diff(log(df_comm$Gold..USD...ozt.[1:120])))
hist(gold_diff_diff1)
gold_diff_diff2 <- diff(diff(log(df_comm$Gold..USD...ozt.[121:240])))
hist(gold_diff_diff2)
#Gold distribution
pval_gold_diff_diff1 <- chiSqTest(gold_diff_diff1)
pval_gold_diff_diff2 <- chiSqTest(gold_diff_diff2)
# QQ plots to see how distribution compares to normal distribution
qqnorm(gold_diff_diff1)
qqline(gold_diff_diff1)
qqnorm(gold_diff_diff2)
qqline(gold_diff_diff2)
# Price Changes
qqnorm(gold_price_change1)
qqline(gold_price_change2)
qqnorm(gold_price_change1)
# Price Changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
qqnorm(gold_price_change2)
qqline(gold_price_change2)
# Log Price Changes/percentage changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
qqnorm(gold_price_change2)
qqline(gold_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(gold_diff_diff1)
qqline(gold_diff_diff1)
qqnorm(gold_diff_diff2)
qqline(gold_diff_diff2)
#*****************
#*Beef
#*****************
hist(df_comm$Beef..USD...kg., breaks=50)
length(df_comm$Beef..USD...kg.)
#histogram of purely values, not time series
hist(df_comm$Beef..USD...kg.)
# First half of the dataset, which is ordered according to time
hist(df_comm$Beef..USD...kg.[1:120])
# Second half (of the time frame)
hist(df_comm$Beef..USD...kg.[121:240])
# price changes/difference in prices
beef_price_change1 <- diff(log(df_comm$Beef..USD...kg.[1:120]))
hist(beef_price_change1)
beef_price_change2 <- diff(log(df_comm$Beef..USD...kg.[121:240]))
hist(beef_price_change2)
# differences in price changes
beef_diff_diff1 <- diff(diff(log(df_comm$Beef..USD...kg.[1:120])))
hist(beef_diff_diff1)
beef_diff_diff2 <- diff(diff(log(df_comm$Beef..USD...kg.[121:240])))
beef(beef_diff_diff2)
hist(beef_diff_diff2)
#Beef distribution
pval_beef_diff_diff1 <- chiSqTest(beef_diff_diff1)
pval_beef_diff_diff2 <- chiSqTest(beef_diff_diff2)
# Log Price Changes/percentage changes
qqnorm(beef_price_change1)
qqline(beef_price_change1)
qqnorm(beef_price_change2)
qqline(beef_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(beef_diff_diff1)
qqline(beef_diff_diff1)
qqnorm(beef_diff_diff2)
qqline(beef_diff_diff2)
df_comm$Date[which(df_comm$rec_indic == 1)]
hist(df_comm$Gold..USD...ozt.(which(df_comm$rec_indic == 1)))
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)], breaks=50)
# Do we need daily data on this...
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$GBeef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)])
# Do we need daily data on this...
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)])
# Refined with more bins
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
# Do we need daily data on this...
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)])
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)])
# Refined with more bins
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Beef..USD...kg.[which(df_comm$rec_indic == 1)], breaks=50)
# Log Price Changes/percentage changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
# Log Price Changes/percentage changes
qqnorm(beef_price_change1)
qqline(beef_price_change1)
qqnorm(beef_price_change2)
qqline(beef_price_change2)
qqnorm(beef_price_change2)
qqline(beef_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(beef_diff_diff1)
qqline(beef_diff_diff1)
qqnorm(beef_diff_diff2)
qqline(beef_diff_diff2)
setwd("/Users/stai/math23c-rproject")
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)], breaks=50)
# Do we need daily data on this...
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)])
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 0)], breaks=50)
hist(df_comm$Gold..USD...ozt.[which(df_comm$rec_indic == 1)], breaks=50)
# Log Price Changes/percentage changes
qqnorm(gold_price_change1)
qqline(gold_price_change1)
qqnorm(gold_price_change2)
qqline(gold_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(gold_diff_diff1)
qqline(gold_diff_diff1)
qqnorm(gold_diff_diff2)
qqline(gold_diff_diff2)
# Log Price Changes/percentage changes
qqnorm(beef_price_change1)
qqline(beef_price_change1)
qqnorm(beef_price_change2) #Note what's happening in the right and left tails.
qqline(beef_price_change2)
# Changes in log price changes/percentage changes ("acceleration")
qqnorm(beef_diff_diff1)
qqline(beef_diff_diff1)
qqnorm(beef_diff_diff2)
qqline(beef_diff_diff2) # Note what's happening in the right and left tails
=======
age=c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37),
hr=c(202,186,187,180,156,169,174,172,153,199,193,174,198,183,178)
)
attach(hr_data)
hr_vs_age <- lm(hr ~ age)
summary(hr_vs_age)
plot(hr_vs_age)
detach(hr_data)
hr_data <- data.frame(
age=c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37),
hr=c(202,186,187,180,156,169,174,172,153,199,193,174,198,183,178)
)
attach(hr_data)
hr_vs_age <- lm(hr ~ age)
summary(hr_vs_age)
plot(hr, age)
plot(hr_vs_age)
detach(hr_data)
hr_data <- data.frame(
age=c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37),
hr=c(202,186,187,180,156,169,174,172,153,199,193,174,198,183,178)
)
attach(hr_data)
hr_vs_age <- lm(hr ~ age)
summary(hr_vs_age)
plot(hr, age)
abline(hr_vs_age)
plot(hr_vs_age)
detach(hr_data)
hr_data <- data.frame(
age=c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37),
hr=c(202,186,187,180,156,169,174,172,153,199,193,174,198,183,178)
)
attach(hr_data)
hr_vs_age <- lm(hr ~ age)
summary(hr_vs_age)
plot(hr, age)
plot(hr_vs_age)
detach(hr_data)
remove(list=ls())
hr_data <- data.frame(
age=c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37),
hr=c(202,186,187,180,156,169,174,172,153,199,193,174,198,183,178)
)
attach(hr_data)
hr_vs_age <- lm(hr ~ age)
summary(hr_vs_age)
plot(hr, age)
plot(hr_vs_age)
detach(hr_data)
bac_data <- data.frame(
beers=c(5,2,9,8,3,7,3,5,3,5),
bac=c(0.10,0.03,0.19,0.12,0.04,0.095,0.07,0.06,0.02,0.05)
)
attach(bac_data)
bac_vs_beers <- lm(beers ~ bac)
summary(bac_vs_beers)
plot(bac, beers)
plot(bac_vs_beers)
detach(bac_data)
pop_data <- data.frame(
year=c(1952,1955,1958,1961,1953,1956,1959),
seals=c(724,1392,1212,1980,176,1392,1672)
)
attach(pop_data)
seals_vs_year <- lm(seals ~ year)
summary(seals_vs_year)
plot(seals, year)
plot(seals_vs_year)
detach(pop_data)
bac_data <- data.frame(
beers=c(5,2,9,8,3,7,3,5,3,5),
bac=c(0.10,0.03,0.19,0.12,0.04,0.095,0.07,0.06,0.02,0.05)
)
attach(bac_data)
bac_vs_beers <- lm(bac ~ beers)
summary(bac_vs_beers)
plot(beers, bac, main="Blood alcohol content vs number of beers consumed")
plot(bac_vs_beers)
detach(bac_data)
pop_data <- data.frame(
year=c(1952,1955,1958,1961,1953,1956,1959),
seals=c(724,1392,1212,1980,176,1392,1672)
)
attach(pop_data)
seals_vs_year <- lm(seals ~ year)
summary(seals_vs_year)
plot(year, seals)
plot(seals_vs_year)
detach(pop_data)
pop_data <- data.frame(
year=c(1952,1955,1958,1961,1953,1956,1959),
seals=c(724,1392,1212,1980,176,1392,1672)
)
attach(pop_data)
seals_vs_year <- lm(seals ~ year)
summary(seals_vs_year)
plot(year, seals, main="Seal population by year")
plot(seals_vs_year)
future_years <- data.frame(lstat=c(1963,2014))
pred <- data.frame(
predict(seals_vs_year,
newdata = future_years,
interval = 'prediction'
)
)
detach(pop_data)
View(future_years)
View(future_years)
pop_data <- data.frame(
year=c(1952,1955,1958,1961,1953,1956,1959),
seals=c(724,1392,1212,1980,176,1392,1672)
)
attach(pop_data)
seals_vs_year <- lm(seals ~ year)
summary(seals_vs_year)
plot(year, seals, main="Seal population by year")
plot(seals_vs_year)
future_years <- data.frame(year=c(1963,2014))
pred <- data.frame(
predict(seals_vs_year,
newdata = future_years,
interval = 'prediction'
)
)
detach(pop_data)
View(pred)
View(pred)
pop_data <- data.frame(
year=c(1952,1955,1958,1961,1953,1956,1959),
seals=c(724,1392,1212,1980,176,1392,1672)
)
attach(pop_data)
seals_vs_year <- lm(seals ~ year)
summary(seals_vs_year)
plot(year, seals, main="Seal population by year")
plot(seals_vs_year)
future_years <- data.frame(year=c(1963,2014))
pred <- data.frame(
predict(seals_vs_year,
newdata = future_years,
interval = 'prediction'
)
); pred
detach(pop_data)
View(seals_vs_year)
View(seals_vs_year)
View(pop_data)
View(pop_data)
pop_data <- data.frame(
year=c(1952,1955,1958,1961,1953,1956,1959,1962,1954,1957,1960),
seals=c(724,1392,1212,1980,176,1392,1672,2116,920,1448,2068)
)
attach(pop_data)
seals_vs_year <- lm(seals ~ year)
summary(seals_vs_year)
plot(year, seals, main="Seal population by year")
plot(seals_vs_year)
future_years <- data.frame(year=c(1963,2014))
pred <- data.frame(
predict(seals_vs_year,
newdata = future_years,
interval = 'prediction'
)
); pred
detach(pop_data)
library(ISLR)
install.packages("ISLR")
library(ISLR)
Auto
linreg <- lm(mpg ~ horsepower, Auto)
linreg
linreg$coefficients
linreg$coefficients$(Intercept)
linreg$coefficients$[(Intercept)]
linreg$coefficients$[1]
linreg$coefficients[1]
linreg
plot(Auto$mpg, Auto$horsepower)
plot(Auto$mpg-39.9359, Auto$horsepower)
plot(Auto$mpg, Auto$horsepower)
plot(Auto$mpg, Auto$horsepower-39.9)
plot(Auto$mpg, Auto$horsepower)
resid(linreg)
plot(Auto$mpg, resid(linreg))
linreg
plot(Auto$mpg, Auto$horsepower)
-log(runif(1,0,1))/2
-log(runif(1,0,1))/2
-log(runif(1,0,1))/2
-log(runif(1,0,1))/2
-log(runif(1,0,1))/2
-log(runif(1,0,1))/2
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(100,0,1)))
mean(-log(runif(1000,0,1)))
mean(-log(runif(1000,0,1)))
mean(-log(runif(100000,0,1)))
mean(-log(runif(100000,0,1)))
mean(-log(runif(100000,0,1)))
mean(-log(runif(1000000,0,1)))
a1 <- c(2,1,0,1)
a2 <- c(1,1,3,2)
a3 <- c(2,0,2,1)
a4 <- c(1,0,4,2)
A <- rbind(a1,a2,a3,a4)
View(A)
View(A)
sum(dbinom(0:20,20,0.6))
sum(rbinom(0:20,20,0.6))
sum(pbinom(0:20,20,0.6, lower.tail = TRUE))
sum(pbinom(0:20,20,0.6, lower.tail = FALSE))
sum((0:20)*pbinom(0:20,20,0.6))
hist(rpois(10000,4), prob = TRUE)
hist(rpois(10000,4), prob = TRUE)
hist(dpois(x,4))
hist(dpois(x,4))
hist(dpois(1:15,4))
barplot(rpois(10000,4))
machine_regression <- lm(expense ~ usage, machine_data)
usage <- c(13,10,20,28,32,17,24,31,40,38)
expense <- c(17,22,30,37,47,30.5,32.5,39,51.5,40)
machine_data <- data.frame(usage=usage, expense=expense)
machine_regression <- lm(expense ~ usage, machine_data)
summary(machine_regression)
log_density <- log(oddbooks$density)
oddbooks$area <- oddbooks$height * oddbooks$breadth
library(DAAG)
oddbooks$area <- oddbooks$height * oddbooks$breadth
oddbooks$volume <- oddbooks$thick * oddbooks$area
oddbooks$density <- oddbooks$weight / oddbooks$volume
log_weight <- log(oddbooks$weight)
log_volume <- log(oddbooks$volume)
plot(log_volume, log_weight)
weight_vs_volume <- lm(log(weight) ~ log(volume), oddbooks)
abline(a=-8.942, b=1.696)
summary(weight_vs_volume)
log_weight <- log(oddbooks$weight)
log_area <- log(oddbooks$area)
plot(log_area, log_weight)
weight_vs_area <- lm(log(weight) ~ log(area), oddbooks)
abline(a=1.697, b=0.793)
summary(weight_vs_area)
log_density <- log(oddbooks$density)
log_volume <- log(oddbooks$volume)
plot(log_volume, log_density)
density_vs_volume <- lm(log(density) ~ log(volume), oddbooks)
abline(a=-8.942, b=1.696) # edit this
summary(density_vs_volume)
log_area <- log(oddbooks$area)
plot(log_area, log_density)
density_vs_area <- lm(log(density) ~ log(area), oddbooks)
abline(a=1.697, b=0.793) # edit this
summary(density_vs_area)
log_density <- log(oddbooks$density)
log_volume <- log(oddbooks$volume)
plot(log_volume, log_density)
density_vs_volume <- lm(log(density) ~ log(volume), oddbooks)
abline(a=-8.942, b=0.696)
summary(density_vs_volume)
log_area <- log(oddbooks$area)
plot(log_area, log_density)
density_vs_area <- lm(log(density) ~ log(area), oddbooks)
abline(a=-5.109, b=0.419)
summary(density_vs_area)
heights <- c(68,64,62,65,66)
weights <- c(132,108,102,115,128)
swimmers <- data.frame(heights=heights, weights=weights)
plot(swimmers)
abline(a=-240.5, b=5.5)
swim_regression <- lm(weights ~ heights, swimmers)
summary(swim_regression)
plot(swim_regression)
oddbooks$area <- oddbooks$height * oddbooks$breadth
oddbooks$volume <- oddbooks$thick * oddbooks$area
oddbooks$density <- oddbooks$weight / oddbooks$volume
log_weight <- log(oddbooks$weight)
log_volume <- log(oddbooks$volume)
plot(log_volume, log_weight)
weight_vs_volume <- lm(log(weight) ~ log(volume), oddbooks)
abline(a=-8.942, b=1.696)
summary(weight_vs_volume)
log_weight <- log(oddbooks$weight)
log_area <- log(oddbooks$area)
plot(log_area, log_weight, main="log(weight) vs log(area)")
weight_vs_area <- lm(log(weight) ~ log(area), oddbooks)
abline(a=1.697, b=0.793)
summary(weight_vs_area)
det(cbind(c(1,-1,-1,-1),c(-1,1,-1,-1),c(-1,-1,1,-1),c(-1,-1,-1,1)))
# This script shows the distributions of month-over-month price changes of our commodities.
library(eeptools) # to import the decomma() function
commodity_prices <- read.csv("source_data/commodities data.csv")
recession_dates <- read.csv("source_data/monthly recession indicator.csv")
recession_dates <- recession_dates[c(-242),] # Removed the entry for March 2021 so that the datasets have congruent date ranges.
commodity_prices[,9] <- decomma(commodity_prices[,9]) # Remove commas from the price of gold column.
goods <- c("Month","Crude_oil", "Sugar", "Soybeans", "Wheat", "Beef", "Rubber", "Cocoa_beans", "Gold", "USD_EUR", "Ice_cream", "Unemployment")
names(commodity_prices) <- goods
# Adding a column to distinguish the three different recessions in our date range.
recession_dates$which_recession <- recession_dates$USREC
recession_dates[84:101,3] <- 2*recession_dates[84:101,3]
recession_dates[230:241,3] <- 3*recession_dates[230:241,3]
# Find month-over-month price changes for each commodity
price_changes <- commodity_prices
for (c in 2:11) {
for (r in 2:241) {
price_changes[r,c] <- commodity_prices[r,c] / commodity_prices[r-1,c] - 1
}
}
price_changes$recession_bool <- recession_dates$USREC
price_changes$which_recession <- recession_dates$which_recession
price_changes <- price_changes[c(-1),]
write.csv(price_changes, "output_files/price changes.csv")
# Display a histogram of the price changes
for (i in 2:11) {
name <- goods[i]
hist(price_changes[,i], main=name)
}
# Repeating that, but during recessions only
for (i in 2:11) {
name <- paste(goods[i],"(recession)")
values <- price_changes[,c(i,13)]
values <- values[values[,2] == 1,]
hist(values[,1], main=name)
}
# Repeat for months not in recession
for (i in 2:11) {
name <- paste(goods[i],"(non-recession)")
values <- price_changes[,c(i,13)]
values <- values[values[,2] == 0,]
hist(values[,1], main=name)
}
setwd("~/GitHub/math23c-rproject")
# This script shows the distributions of month-over-month price changes of our commodities.
library(eeptools) # to import the decomma() function
commodity_prices <- read.csv("source_data/commodities data.csv")
recession_dates <- read.csv("source_data/monthly recession indicator.csv")
recession_dates <- recession_dates[c(-242),] # Removed the entry for March 2021 so that the datasets have congruent date ranges.
commodity_prices[,9] <- decomma(commodity_prices[,9]) # Remove commas from the price of gold column.
goods <- c("Month","Crude_oil", "Sugar", "Soybeans", "Wheat", "Beef", "Rubber", "Cocoa_beans", "Gold", "USD_EUR", "Ice_cream", "Unemployment")
names(commodity_prices) <- goods
# Adding a column to distinguish the three different recessions in our date range.
recession_dates$which_recession <- recession_dates$USREC
recession_dates[84:101,3] <- 2*recession_dates[84:101,3]
recession_dates[230:241,3] <- 3*recession_dates[230:241,3]
# Find month-over-month price changes for each commodity
price_changes <- commodity_prices
for (c in 2:11) {
for (r in 2:241) {
price_changes[r,c] <- commodity_prices[r,c] / commodity_prices[r-1,c] - 1
}
}
price_changes$recession_bool <- recession_dates$USREC
price_changes$which_recession <- recession_dates$which_recession
price_changes <- price_changes[c(-1),]
write.csv(price_changes, "output_files/price changes.csv")
# Display a histogram of the price changes
for (i in 2:11) {
name <- goods[i]
hist(price_changes[,i], main=name)
}
# Repeating that, but during recessions only
for (i in 2:11) {
name <- paste(goods[i],"(recession)")
values <- price_changes[,c(i,13)]
values <- values[values[,2] == 1,]
hist(values[,1], main=name)
}
# Repeat for months not in recession
for (i in 2:11) {
name <- paste(goods[i],"(non-recession)")
values <- price_changes[,c(i,13)]
values <- values[values[,2] == 0,]
hist(values[,1], main=name)
}
# Robi's goods: sugar, cocoa, usd:eur, ice cream, unemployment
View(price_changes)
View(price_changes)
View(values)
View(values)
price_changes <- read.csv("source_data/price changes.csv")
View(price_changes)
View(price_changes)
View(price_changes)
View(price_changes)
glm(recession_bool ~ soybeans, data = price_changes, family = 'binomial')
glm(recession_bool ~ Soybeans, data = price_changes, family = 'binomial')
soybeans_regression <- glm(recession_bool ~ Soybeans, data = price_changes, family = 'binomial')
summary(soybeans_regression)
library(boot)
library(caret)
plot(price_changes$Soybeans, price_changes$recession_bool)
curve(inv.logit(1.5869x-1.6825), add=TRUE)
curve(function(x) inv.logit(1.5869x-1.6825), add=TRUE)
formula <- function(x) inv.logit(1.5869x-1.6825)
curve(inv.logit(1.5869*x-1.6825), add=TRUE)
curve(inv.logit(1.5869*x-1.6825), from = -3, to=3)
curve(inv.logit(1.5869*x-1.6825), from = -3, to = 4)
soybeans_regression <- glm(recession_bool ~ Soybeans, data = price_changes, family = 'binomial')
summary(soybeans_regression)
plot(price_changes$Soybeans, price_changes$recession_bool)
gold_regression <- glm(recession_bool ~ Gold, data = price_changes, family = 'binomial')
summary(gold_regression)
plot(price_changes$Gold, price_changes$recession_bool)
goods_vs_recession_logistic_model <- glm(
recession_bool ~ crude_oil + Sugar + Soybeans + Wheat + Beef + Rubber + Cocoa_beans + Gold + Ice_cream,
data = price_changes, family = 'binomial')
goods_vs_recession_logistic_model <- glm(
recession_bool ~ Crude_oil + Sugar + Soybeans + Wheat + Beef + Rubber + Cocoa_beans + Gold + Ice_cream,
data = price_changes, family = 'binomial')
summary(goods_vs_recession_logistic_model)
summary(soybeans_regression)
summary(gold_regression)
goods_vs_recession_logistic_model <- glm(
recession_bool ~ Crude_oil + Sugar + Soybeans + Wheat + Beef + Rubber + Cocoa_beans + Gold + Ice_cream,
data = price_changes, family = 'binomial')
summary(goods_vs_recession_logistic_model)
negative_goods_regression <- glm(recession_bool ~ Crude_oil + Wheat + Rubber + Ice_cream, data = price_changes, family = 'binomial')
summary(negative_goods_regression)
goods_vs_recession_logistic_model <- glm(
recession_bool ~ Crude_oil + Sugar + Soybeans + Wheat + Beef + Rubber + Cocoa_beans + Gold + USD_EUR + Ice_cream,
data = price_changes, family = 'binomial')
summary(goods_vs_recession_logistic_model)
summary(soybeans_regression)
goods_vs_recession_logistic_model <- glm(
recession_bool ~ Crude_oil + Sugar + Soybeans + Wheat + Beef + Rubber + Cocoa_beans + Gold + USD_EUR + Ice_cream,
data = price_changes, family = 'binomial')
summary(goods_vs_recession_logistic_model)
negative_goods_regression <- glm(recession_bool ~ Crude_oil + Wheat + Rubber + Ice_cream, data = price_changes, family = 'binomial')
summary(negative_goods_regression)
icecream_regression <- glm(recession_bool ~ Ice_cream, data = price_changes, family = 'binomial')
summary(icecream_regression)
plot(price_changes$Ice_cream, price_changes$recession_bool)
curve(inv.logit(-1.5900*x-1.6687), add=TRUE)
plot.window(c(-1,1), c(0,1))
plot(price_changes$Ice_cream, price_changes$recession_bool)
curve(inv.logit(-1.5900*x-1.6687), add=TRUE)
plot(price_changes$Ice_cream, price_changes$recession_bool)
plot.window(c(-1,1), c(0,1))
curve(inv.logit(-1.5900*x-1.6687), add=TRUE)
plot(price_changes$Ice_cream, price_changes$recession_bool)
curve(inv.logit(-1.5900*x-1.6687), add=TRUE)
negative_goods_regression <- glm(recession_bool ~ Crude_oil + Wheat + Rubber + Ice_cream, data = price_changes, family = 'binomial')
summary(negative_goods_regression)
# Probability of a recession if all of these goods lost 100% of their value this month:
inv.logit(-1.6582-0.2640-0.7643-0.8586-1.5616)
# Probability of a recession if all of these goods lost 100% of their value this month:
inv.logit(-1.6582+0.2640+0.7643+0.8586+1.5616)
# Probability of a recession if all of these goods doubled in price last month:
inv.logit(-1.6582-0.2640-0.7643-0.8586-1.5616)
icecream_regression <- glm(recession_bool ~ Ice_cream, data = price_changes, family = 'binomial')
summary(icecream_regression)
plot(price_changes$Ice_cream, price_changes$recession_bool)
curve(inv.logit(-1.5900*x-1.6687), add=TRUE)
inv.logit(-1.6687)
icecream_predictions <- as.factor(predict(icecream_regression, newdata=price_changes$Ice_cream, type='response') > inv.logit(-1.6687))
predict(icecream_regression, newdata=price_changes$Ice_cream, type='response')
icecream_predictions <- as.factor(predict(icecream_regression, newdata=price_changes, type='response') > inv.logit(-1.6687))
confusionMatrix(icecream_predictions, reference = as.factor(price_changes$recession_bool==1))
summary(negative_goods_regression)
negative_goods_predictions <- as.factor(predict(negative_goods_regression, newdata=price_changes, type='response') > inv.logit(-1.6582))
confusionMatrix(negative_goods_predictions, reference = as.factor(price_changes$recession_bool==1))
negative_goods_predictions <- as.factor(predict(negative_goods_regression, newdata=price_changes, type='response') > 0.5)
confusionMatrix(negative_goods_predictions, reference = as.factor(price_changes$recession_bool==1))
negative_goods_predictions <- as.factor(predict(negative_goods_regression, newdata=price_changes, type='response') > 0.5)
confusionMatrix(negative_goods_predictions, reference = as.factor(price_changes$recession_bool==1))
inv.logit(-1.6687)
icecream_predictions <- as.factor(predict(icecream_regression, newdata=price_changes, type='response') > 0.5)
confusionMatrix(icecream_predictions, reference = as.factor(price_changes$recession_bool==1))
negative_goods_predictions <- as.factor(predict(negative_goods_regression, newdata=price_changes, type='response') > 0.5)
confusionMatrix(negative_goods_predictions, reference = as.factor(price_changes$recession_bool==1))
recession_predictions <- as.factor(predict(goods_vs_recession_logistic_model, newdata=price_changes, type='response') > 0.5)
confusionMatrix(recession_predictions, reference = as.factor(price_changes$recession_bool==1))
summary(goods_vs_recession_logistic_model)
recession_predictions <- as.factor(predict(goods_vs_recession_logistic_model, newdata=price_changes, type='response') > inv.logit(-1.7006))
confusionMatrix(recession_predictions, reference = as.factor(price_changes$recession_bool==1))
price_changes <- read.csv("source_data/price changes.csv")
soybeans_regression <- glm(recession_bool ~ Soybeans, data = price_changes, family = 'binomial')
soybeans_regression <- glm(recession_bool ~ Soybeans, data = price_changes, family = 'binomial')
summary(soybeans_regression)
plot(price_changes$Soybeans, price_changes$recession_bool)
goods_vs_recession_logistic_model <- glm(
recession_bool ~ Crude_oil + Sugar + Soybeans + Wheat + Beef + Rubber + Cocoa_beans + Gold + USD_EUR + Ice_cream,
data = price_changes, family = 'binomial')
summary(goods_vs_recession_logistic_model)
negative_goods_regression <- glm(recession_bool ~ Crude_oil + Wheat + Rubber + Ice_cream, data = price_changes, family = 'binomial')
summary(negative_goods_regression)
# Probability of a recession if all of these goods lost 100% of their value this month:
inv.logit(-1.6582+0.2640+0.7643+0.8586+1.5616)
# Probability of a recession if all of these goods doubled in price last month:
inv.logit(-1.6582-0.2640-0.7643-0.8586-1.5616)
icecream_regression <- glm(recession_bool ~ Ice_cream, data = price_changes, family = 'binomial')
summary(icecream_regression)
plot(price_changes$Ice_cream, price_changes$recession_bool)
curve(inv.logit(-1.5900*x-1.6687), add=TRUE)
icecream_predictions <- as.factor(predict(icecream_regression, newdata=price_changes, type='response') > 0.5)
confusionMatrix(icecream_predictions, reference = as.factor(price_changes$recession_bool==1))
summary(icecream_regression)
inv.logit(-1.6687)
icecream_predictions <- as.factor(predict(icecream_regression, newdata=price_changes, type='response') > inv.logit(-1.6687))
confusionMatrix(icecream_predictions, reference = as.factor(price_changes$recession_bool==1))
View(price_changes)
View(price_changes)
>>>>>>> 666ef32d4cd1c5fa2ee9945d295cf52abbf996bb
